{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c801069d",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "• All questions are compulsory.\n",
    "• In each of the questions you have to automate the process. You do not have to click on any button, click any clickable element, enter keywords in search boxes manually. Each process has to be performed via coding.\n",
    "• Q1 and Q2 are connected questions i.e. after attempting Q1 proceed to Q2. Do not write whole code from beginning for Q2.\n",
    "• You may use any web scraping library and tools.\n",
    "• The question can be attempted in various ways; the correctness of question depends on the output.\n",
    "• If you encounter any Null values during scraping, you may replace it by hyphen.\n",
    "Exercise:\n",
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa86dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding element for search bar\n",
    "search_f=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "search_f.send_keys(\"guitar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d041d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "bname=[]\n",
    "name=[]\n",
    "pric=[]\n",
    "returnn=[]\n",
    "expdev=[]\n",
    "avail=[]\n",
    "prurl=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract brand name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract name\n",
    "nametags=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "nametags[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a469a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nametags:\n",
    "    nam=i.text\n",
    "    name.append(nam)\n",
    "name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract price\n",
    "ptags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "ptags[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ptags:\n",
    "    pp=i.text\n",
    "    pric.append(pp)\n",
    "pric[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract return/exchange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract expected delivery\n",
    "dtags=driver.find_elements_by_xpath(\"//span[@class='a-color-base a-text-bold']\")\n",
    "dtags[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dtags:\n",
    "    pp=i.text\n",
    "    expdev.append(pp)\n",
    "expdev[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7345f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract Availability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract product url\n",
    "dptags=driver.find_elements_by_xpath(\"//div[@class='a-section aok-relative s-image-square-aspect']\")\n",
    "dptags[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dptags:\n",
    "    pp=i.text\n",
    "    prurl.append(pp)\n",
    "prurl[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1a18d",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all the Product URLS\n",
    "urls = []\n",
    "for i in range(0,3):\n",
    "    Page_urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-no-outline']\")#collecting urls of all the laptop\n",
    "    for i in Page_urls:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "        \n",
    "    #next button \n",
    "    nxt_btn=driver.find_element_by_xpath(\"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\") \n",
    "    url=nxt_btn.get_attribute('href')\n",
    "    driver.get(url)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc791093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making empty lists and scraping the required spots\n",
    "Product_name = []\n",
    "Brand_name= []\n",
    "Ratings = []\n",
    "No_Ratings = []\n",
    "Price = []\n",
    "Return = []\n",
    "Expected_Delivery = []\n",
    "Availability = []\n",
    "Other_Details = []\n",
    "\n",
    "#Start with for loop\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Scraping data for product name\n",
    "    try:\n",
    "        prod=driver.find_element_by_xpath(\"//span[@id='productTitle']\")\n",
    "        Product_name.append(prod.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Product_name.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for brand name\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath(\"//div[@id='bylineInfo_feature_div']/div/a\")\n",
    "        Brand_name.append(brand.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Brand_name.append(\"-\")\n",
    "        \n",
    "        \n",
    "     #Scraping data for Ratings\n",
    "    try:\n",
    "        rat=driver.find_element_by_xpath(\"//span[@id='acrPopover']\")\n",
    "        Ratings.append(rat.get_attribute(\"title\"))   \n",
    "    except NoSuchElementException as e:\n",
    "        Ratings.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for No of Ratings\n",
    "    try:\n",
    "        no_rat=driver.find_element_by_xpath(\"//a[@id='acrCustomerReviewLink']/span\")\n",
    "        No_Ratings.append(no_rat.text)\n",
    "    except NoSuchElementException as e:\n",
    "        No_Ratings.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Price\n",
    "    try:\n",
    "        pri=driver.find_element_by_xpath(\"//span[@id='priceblock_ourprice']\")\n",
    "        Price.append(pri.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Price.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Return/Exchange\n",
    "    try:\n",
    "        ret=driver.find_element_by_xpath(\"//div[@data-name='RETURNS_POLICY']/span/div[2]/a\")\n",
    "        Return.append(ret.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Return.append(\"-\")\n",
    "        \n",
    "     \n",
    "    #Scraping data for Expected_Delivary\n",
    "    try:\n",
    "        delivary=driver.find_element_by_xpath(\"//div[@id='ddmDeliveryMessage']/b\")\n",
    "        Expected_Delivery.append(delivary.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Expected_Delivery.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Availability\n",
    "    try:\n",
    "        avai=driver.find_element_by_xpath(\"//div[@id='availability']/span\")\n",
    "        Availability.append(avai.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Availability.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Other_Details\n",
    "    try:\n",
    "        details=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']\")\n",
    "        Other_Details.append(details.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Other_Details.append(\"-\")\n",
    "        \n",
    "        \n",
    "#DATA FRAMEING\n",
    "Laptop=pd.DataFrame({})\n",
    "Laptop['Name'] = Product_name\n",
    "Laptop['Brand'] = Brand_name\n",
    "Laptop['Rating'] = Ratings\n",
    "Laptop['No of Ratings'] = No_Ratings\n",
    "Laptop['Price'] = Price\n",
    "Laptop['Return'] = Return\n",
    "Laptop['Expected_Delivery'] = Expected_Delivery\n",
    "Laptop['Availability'] = Availability\n",
    "Laptop['Other_Details'] = Other_Details\n",
    "Laptop['Urls'] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2199dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the dataframe\n",
    "Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9aaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1734ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f38d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f20a1b",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser with specified url\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "# getting images.google.com\n",
    "url = \"https://images.google.com/\"\n",
    "#Creating empty list and giving search items as list and creating loop\n",
    "urls = []    \n",
    "data = []\n",
    "search_item = [\"fruits\", \"cars\", \"Machine Learning\"]\n",
    "for item in search_item:\n",
    "    driver.get(url)  \n",
    "    time.sleep(5)\n",
    "    search_bar = driver.find_element_by_tag_name(\"input\") #Xpath for search bar\n",
    "    \n",
    "    search_bar.send_keys(str(item))      #sending key word for search item\n",
    "    \n",
    "    search_button =driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\").click() #Clicking on search button\n",
    "    \n",
    "    # scrolling the web page to get more images\n",
    "    for _ in range(500):\n",
    "        driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "        \n",
    "        imgs = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "                if(source[0:4] == 'http'):\n",
    "                    img_url.append(source)\n",
    "    for i in img_url[:100]:\n",
    "        urls.append(i)\n",
    "                    \n",
    "for i in range(len(urls)):\n",
    "    if i >= 300:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 300))\n",
    "    response = requests.get(urls[i])\n",
    "\n",
    "    file = open(r\"C:\\Users\\Asus\\Flipwork\\GoogleImages\"+str(i)+\".jpg\", \"wb\")\n",
    "\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb919c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eeb309",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d467b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "item = input(\" Enter the name of Smartphone that has to be searched : \")\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the web page with given url\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on login close icon\n",
    "login_X_btn = driver.find_element_by_xpath(\"//div[@class='_2QfC02']//button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving input key word to search bar\n",
    "serch_bar = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "serch_bar.send_keys(item)\n",
    "\n",
    "srch_btn = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "srch_btn.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching urls of phones coming on 1st page\n",
    "page1_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    page1_urls.append(url.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page1_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smartphones = {}\n",
    "Smartphones[\"Brand\"] = []\n",
    "Smartphones[\"Phone name\"] = []\n",
    "Smartphones[\"Colour\"] = []\n",
    "Smartphones[\"RAM\"] = []\n",
    "Smartphones[\"Storage(ROM)\"] = []\n",
    "Smartphones[\"Primary Camera\"] = []\n",
    "Smartphones[\"Secondary Camera\"] = []\n",
    "Smartphones[\"Display Size\"] = []\n",
    "Smartphones[\"Display Resolution\"] = []\n",
    "Smartphones[\"Processor\"] = []\n",
    "Smartphones[\"Processor Cores\"] = []\n",
    "Smartphones[\"Battery Capacity\"] = []\n",
    "Smartphones[\"Price\"] = []\n",
    "Smartphones[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42228b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from each url of page 1\n",
    "for url in page1_urls:\n",
    "    driver.get(url)                                                        \n",
    "    print(\"Scraping URL = \", url)\n",
    "    Smartphones['URL'].append(url)                                                          \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Clicking on read more button\n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     \n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "    \n",
    "    \n",
    "    #Scraping brand name of phone data\n",
    "    try:\n",
    "        brand_tags = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      \n",
    "        Smartphones[\"Brand\"].append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Brand'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone name data\n",
    "    try:\n",
    "        name_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')     \n",
    "        Smartphones['Phone name'].append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Phone name'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone color data\n",
    "    try:\n",
    "        color_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      \n",
    "        Smartphones['Colour'].append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Colour'].append('-')\n",
    "     \n",
    "    \n",
    "    #Scraping RAM data\n",
    "    try:\n",
    "        ram_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                \n",
    "        Smartphones['RAM'].append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['RAM'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping ROM data\n",
    "    try:\n",
    "        rom_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')        \n",
    "        Smartphones['Storage(ROM)'].append(rom_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Storage(ROM)'].append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping Primary camera data\n",
    "    try:                                                                                    \n",
    "        pri_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        Smartphones['Primary Camera'].append(pri_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Primary Camera'].append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping secondary camera data\n",
    "    try:                                                                                    \n",
    "        sec_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if sec_tags != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        Smartphones['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Secondary Camera'].append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping Display size data \n",
    "    try:\n",
    "        disp_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_tags.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  \n",
    "        Smartphones['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Size'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping display resolution data\n",
    "    try:\n",
    "        dires_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if dires_tags.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_reso = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')    \n",
    "        Smartphones['Display Resolution'].append(disp_reso.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Resolution'].append('-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Scraping Processor data\n",
    "    try:\n",
    "        pro_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_tags.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   \n",
    "        Smartphones['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor'].append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Scraping Processor core data    \n",
    "    try:                                                                                     \n",
    "        core_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_tags.text != \"Processor Core\" :\n",
    "            core_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_tags.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        Smartphones['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor Cores'].append('-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Scraping battery capacity data\n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                \n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_capa = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')              \n",
    "        Smartphones['Battery Capacity'].append(bat_capa.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Battery Capacity'].append('-')\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Scraping Price data\n",
    "    try:\n",
    "        price_tags = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      \n",
    "        Smartphones['Price'].append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Price'].append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958898cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking lengths of all scraped data\n",
    "print(len(Smartphones[\"Brand\"]), len(Smartphones[\"Phone name\"]), len(Smartphones[\"Colour\"]), len(Smartphones[\"RAM\"]), len(Smartphones[\"Storage(ROM)\"]), len(Smartphones[\"Primary Camera\"]), len(Smartphones[\"Secondary Camera\"]), len(Smartphones[\"Display Size\"]), len(Smartphones[\"Display Resolution\"]), len(Smartphones[\"Processor\"]), len(Smartphones[\"Processor Cores\"]), len(Smartphones[\"Battery Capacity\"]), len(Smartphones[\"Price\"]), len(Smartphones['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e12250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Framing the data\n",
    "df = pd.DataFrame.from_dict(Smartphones)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44483494",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c95e4",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening google maps web page\n",
    "url = \"https://www.google.co.in/maps\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#Sending keyword for seach bar and search button\n",
    "city = input('Enter City name that has to be searched : ')\n",
    "search_bar = driver.find_element_by_id(\"searchboxinput\")                       \n",
    "search_bar.clear()                                                             \n",
    "time.sleep(2)\n",
    "search_bar.send_keys(city)                                                     \n",
    "search_btn = driver.find_element_by_id(\"searchbox-searchbutton\")              \n",
    "search_btn.click()                                                             \n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_str = driver.current_url\n",
    "    print(\"URL Extracted: \", url_str)\n",
    "    latitude_longitude = re.findall(r'@(.*)data',url_str)\n",
    "    if len(latitude_longitude):\n",
    "        lat_lng_list = latitude_longitude[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            latitude = lat_lng_list[0]\n",
    "            longitude = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(latitude, longitude))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc829a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee8ed3",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver.get('https://trak.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting xpath for funding deals\n",
    "fund_button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(fund_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a007f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "fund_deals = {}\n",
    "fund_deals['Date'] = []\n",
    "fund_deals['Startup Name'] = []\n",
    "fund_deals['Industry OR Vertical'] = []\n",
    "fund_deals['Sub-Vertical'] = []\n",
    "fund_deals['Location'] = []\n",
    "fund_deals['Investor'] = []\n",
    "fund_deals['Investment Type'] = []\n",
    "fund_deals['Amount(in USD)'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on tablepress\n",
    "for i in range(48,51):\n",
    "    driver.find_element_by_xpath('//div[@id=\"tablepress-{}_wrapper\"]/div/label/select/option[4]'.format(i)).click()\n",
    "\n",
    "    # Scraping data of Date\n",
    "    date_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for date in date_tags:\n",
    "        fund_deals['Date'].append(date.text)\n",
    "\n",
    "    # Scraping data of Startup Name\n",
    "    name_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for name in name_tags:\n",
    "        fund_deals['Startup Name'].append(name.text)\n",
    "    \n",
    "    # Scraping data of Industry OR Vertical\n",
    "    ind_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind_tags:\n",
    "        fund_deals['Industry OR Vertical'].append(n.text)\n",
    "    \n",
    "    # Scraping data of Sub-Vertical\n",
    "    sv_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for sv in sv_tags:\n",
    "        fund_deals['Sub-Vertical'].append(sv.text)\n",
    "\n",
    "    # Scraping data of Location\n",
    "    loc_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for loc in loc_tags:\n",
    "        fund_deals['Location'].append(loc.text)\n",
    "    \n",
    "    # Scraping data of Investor\n",
    "    inv_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for inv in inv_tags:\n",
    "        fund_deals['Investor'].append(inv.text)\n",
    "        \n",
    "    # Scraping data of Investment Type\n",
    "    invt_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for invt in invt_tags:\n",
    "        fund_deals['Investment Type'].append(invt.text)\n",
    "    \n",
    "    # Scraping data of Amount\n",
    "    amt_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for amt in amt_tags:\n",
    "        fund_deals['Amount(in USD)'].append(amt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Framimg the scraped data\n",
    "fund_df = pd.DataFrame(fund_deals)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbdf873",
   "metadata": {},
   "source": [
    "# 7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the specified url\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "#searching for best laptop\n",
    "best_gam_lap = driver.find_element_by_xpath(\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "lap_name = []\n",
    "ope_sys = []\n",
    "display = []\n",
    "processor = []\n",
    "memory = []\n",
    "weight = []\n",
    "dimensions = []\n",
    "graph_proc = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a822812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the data of laptop names\n",
    "name_tags = driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[1]\")\n",
    "for name in name_tags:\n",
    "    lap_name.append(name.text)\n",
    "    \n",
    "    \n",
    "#Scraping the data of operating system\n",
    "try:\n",
    "    os_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[3]//td[3]\")\n",
    "    for os in os_tags:\n",
    "        ope_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of display\n",
    "try:\n",
    "    disp_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[4]//td[3]\")\n",
    "    for disp in disp_tags:\n",
    "        display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of Processor\n",
    "try:\n",
    "    pro_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[5]//td[3]\")\n",
    "    for pro in pro_tags:\n",
    "        processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of memory\n",
    "try:\n",
    "    memo_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[6]//td[3]\")\n",
    "    for memo in memo_tags:\n",
    "        memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of weight\n",
    "try:\n",
    "    wgt_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[7]//td[3]\")\n",
    "    for wgt in wgt_tags:\n",
    "        weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of dimensions\n",
    "try:\n",
    "    dim_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[8]//td[3]\")\n",
    "    for dim in dim_tags:\n",
    "        dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of Graph processor\n",
    "try:\n",
    "    gra_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[9]//td[3]\")\n",
    "    for gra in gra_tags:\n",
    "        graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of price\n",
    "try:\n",
    "    pri_tags = driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "    for pri in pri_tags:\n",
    "        price.append(pri.text.replace('₹','Rs '))\n",
    "except NoSuchElementException:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94180556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "Gaming_Laptop['Laptop Name'] = lap_name\n",
    "Gaming_Laptop['Operating system'] = ope_sys\n",
    "Gaming_Laptop['Display'] = display\n",
    "Gaming_Laptop['Processor'] = processor\n",
    "Gaming_Laptop['Memory'] = memory\n",
    "Gaming_Laptop['Weight'] = weight\n",
    "Gaming_Laptop['Dimensions'] = dimensions\n",
    "Gaming_Laptop['Graphical Processor'] = graph_proc\n",
    "Gaming_Laptop['Price'] = price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Gaming_Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e357dd9",
   "metadata": {},
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c282b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.forbes.com/billionaires/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[]\n",
    "rank=[]\n",
    "worth=[]\n",
    "age=[]\n",
    "citizen=[]\n",
    "source=[]\n",
    "industry=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract Names\n",
    "titles_tags=driver.find_elements_by_xpath(\"//div[@class='personName']\")\n",
    "titles_tags[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518289e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    titles.append(title)\n",
    "titles[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66772d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract rank\n",
    "titles_tags1=driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "titles_tags1[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags1:\n",
    "    title=i.text\n",
    "    rank.append(title)\n",
    "rank[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract worth\n",
    "titles_tags2=driver.find_elements_by_xpath(\"//div[@class='netWorth']\")\n",
    "titles_tags2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da82734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags2:\n",
    "    title=i.text\n",
    "    worth.append(title)\n",
    "worth[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract age\n",
    "titles_tags3=driver.find_elements_by_xpath(\"//div[@class='age']\")\n",
    "titles_tags3[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e935ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags3:\n",
    "    title=i.text\n",
    "    age.append(title)\n",
    "age[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract citizenship\n",
    "titles_tags4=driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "titles_tags4[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags4:\n",
    "    title=i.text\n",
    "    citizen.append(title)\n",
    "citizen[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract source\n",
    "titles_tags5=driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "titles_tags5[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags5:\n",
    "    title=i.text\n",
    "    source.append(title)\n",
    "source[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract industry\n",
    "titles_tags6=driver.find_elements_by_xpath(\"//div[@class='category']\")\n",
    "titles_tags6[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fe768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags6:\n",
    "    title=i.text\n",
    "    industry.append(title)\n",
    "industry[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=pd.DataFrame({})\n",
    "jobs['Name']=titles\n",
    "jobs['Rank']=rank\n",
    "jobs['Age']=age\n",
    "jobs['Net Worth']=worth\n",
    "jobs['Country']=citizen\n",
    "jobs['Source']=source\n",
    "jobs['Industry']=industry\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7d36f",
   "metadata": {},
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f321f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening youtube\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd624cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding element for search bar\n",
    "search_bar = driver.find_element_by_id('search')\n",
    "search_bar.send_keys(\"GOT\")  #entering Video name\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding element for search bar\n",
    "search_bar = driver.find_element_by_id('search')\n",
    "search_bar.send_keys(\"GOT\")  #entering Video name\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")  \n",
    "search_btn.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on first video\n",
    "link_click = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 time we scroll down to generate more Comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3637d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the data of comments\n",
    "cm_tags = driver.find_elements_by_id(\"content-text\")\n",
    "for cm in cm_tags:\n",
    "    if cm.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(cm.text)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data of time when comment was posted\n",
    "tm_tags = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for tm in tm_tags:\n",
    "    Time.append(tm.text)\n",
    "\n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f42203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data of comment likes\n",
    "like_tags = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for like in like_tags:\n",
    "    Likes.append(like.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534994cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "Youtube=pd.DataFrame({})\n",
    "Youtube['Comments'] = comments\n",
    "Youtube['Comment_time'] = comment_time\n",
    "Youtube['Comment upvotes'] = No_of_Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad3400",
   "metadata": {},
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "#get the web page with given url\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "search_loc = driver.find_element_by_id('search-input-field')\n",
    "# write Lonodn in search bar\n",
    "search_loc.send_keys(\"London\")\n",
    "time.sleep(5)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "london.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# do click on search button\n",
    "search_btn = driver.find_element_by_id('search-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88175d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the requered informations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    #fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "    #fetching distance from city centre\n",
    "    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "    #fetch privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "    #fetching dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            #fetching facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    #lets fetch url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #lets click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    #fetching ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    #fetching total reviews\n",
    "        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        #fetch overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    #fetch property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc620d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "dff = pd.DataFrame({})\n",
    "dff['Hostel_Name'] = hostel_name\n",
    "dff['Distance fron city centre'] = distance\n",
    "dff['Ratings'] = rating\n",
    "dff['Total_reviews'] = reviews\n",
    "dff['Overall Reviews'] = over_all\n",
    "dff['Privates from price'] = pvt_prices\n",
    "dff['Dorms from price'] = dorms_price\n",
    "dff['Facilities'] = facilities[:83]\n",
    "dff['Description'] = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5288482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085304b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
