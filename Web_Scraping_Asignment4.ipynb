{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia:Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/You need to find following details:\n",
    "- A) Rank\n",
    "- B) Name\n",
    "- C) Artist\n",
    "- D) Upload date\n",
    "- E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Upload_date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of rank details\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[1]\")[:30]:\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of video names\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[2]\")[:30]:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of artist name\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[3]\")[:30]:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:        \n",
    "    Artist.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of upload dates\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[5]\")[:30]:\n",
    "        Upload_date.append(i.text)\n",
    "except NoSuchElementException:        \n",
    "    Upload_date.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of views\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[4]\")[:30]:\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Youtube=pd.DataFrame({})\n",
    "Youtube['Rank'] = Rank\n",
    "Youtube['Name'] = Name\n",
    "Youtube['Artist'] = Artist\n",
    "Youtube['Uploaded Date'] = Upload_date\n",
    "Youtube['Views'] = Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv. Url = https://www.bcci.tv/. You need to find following details:\n",
    "- A) Match title (I.e. 1st ODI)\n",
    "- B) Series\n",
    "- C) Place\n",
    "- D) Date\n",
    "- E) Time\n",
    "\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting url for international fixture and clicking on it\n",
    "Options_btn = driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']\").click()\n",
    "\n",
    "Infix_btn = driver.find_element_by_xpath(\"//a[@class='navigation__link navigation__link--in-drop-down']\").click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Match_title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of match title\n",
    "for i in driver.find_elements_by_xpath(\"//strong[@class='fixture__name fixture__name--with-margin']\"):\n",
    "    Match_title.append(i.text)\n",
    "    \n",
    "#Scraping data of Place\n",
    "for i in driver.find_elements_by_xpath(\"//p[@class='fixture__additional-info']//span\"):\n",
    "    Place.append(i.text)\n",
    "    \n",
    "#for scraping date and time we have go into each title    \n",
    "#Scraping url for each match\n",
    "mat_url = []\n",
    "urls = driver.find_elements_by_xpath(\"//div[@class='js-list']//a\")\n",
    "for i in urls:\n",
    "    mat_url.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "#Now scraping date, time and series\n",
    "for i in mat_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(8)\n",
    "    \n",
    "    \n",
    "    #Scraping data of Series\n",
    "    try:\n",
    "        series = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[2]/section[3]/div/ul/li[1]/span[2]\")\n",
    "        Series.append(series.text)\n",
    "    except NoSuchElementException:\n",
    "        Series.append('-')\n",
    "        \n",
    "    #Scraping data of dates\n",
    "    try:\n",
    "        date = driver.find_element_by_xpath(\"//div[@class='mc-header-scorebox__datetime']//strong\")\n",
    "        Date.append(date.text)\n",
    "    except NoSuchElementException:\n",
    "        Date.append('-')\n",
    "        \n",
    "    #Scraping data of time\n",
    "    try:\n",
    "        times = driver.find_element_by_xpath(\"//span[@class='mc-header-scorebox__ist-time']\")\n",
    "        Time.append(times.text)\n",
    "    except NoSuchElementException:\n",
    "        Time.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "BCCI=pd.DataFrame({})\n",
    "BCCI['Match Title'] = Match_title\n",
    "BCCI['Series'] = Series\n",
    "BCCI['Place'] = Place\n",
    "BCCI['Date'] = Date\n",
    "BCCI['Time'] = Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "BCCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/ You need to find following details:\n",
    "- A) Name\n",
    "- B) Description\n",
    "\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.guru99.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going to search bar and searching for selenium exception\n",
    "ser_bar = driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[1]/div/table/tbody/tr/td[1]/input\")\n",
    "ser_bar.send_keys(\"selenium exception handling\")\n",
    "ser_but = driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[2]/button\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Url of selenium exception handling page\n",
    "urls = driver.find_element_by_xpath(\"//div[@class='gsc-expansionArea']//div//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping data of names of exceptions\n",
    "Name = []\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']//tr//td[1]\")[1:]:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping data of descriptions of exceptions  \n",
    "Description = []\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']//tr//td[2]\")[1:]:\n",
    "    Description.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Guru=pd.DataFrame({})\n",
    "Guru['Name of Exception'] = Name\n",
    "Guru['Description of Exception'] = Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Guru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/ You have to find following details:\n",
    "- A) Rank\n",
    "- B) State\n",
    "- C) GSDP at current price (19-20)\n",
    "- D) GSDP at current price (18-19)\n",
    "- E) Share(18-19)\n",
    "- F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on economy section\n",
    "economy = driver.find_element_by_xpath(\"//div[@class='navbar']//div[2]//button\").click()\n",
    "\n",
    "urls = driver.find_element_by_xpath(\"//div[@class='dropdown-content']//a[3]\")\n",
    "ineco_page = urls.get_attribute(\"href\")\n",
    "#Going to indian economy page\n",
    "driver.get(ineco_page)  \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP_1 = []\n",
    "GSDP_2 = []\n",
    "Share = []\n",
    "GDP = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the url of page containing GDP of indian states\n",
    "sta_GDP = driver.find_element_by_xpath(\"//ul[@style='list-style-type:none;margin-left:20px;']/li/a\")\n",
    "GDP_url = sta_GDP.get_attribute(\"href\")\n",
    "\n",
    "driver.get(GDP_url)\n",
    "time.sleep(6)\n",
    "\n",
    "\n",
    "#Scraping data of rank\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[1]\"):\n",
    "    Rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of state name\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[2]\"):\n",
    "    State.append(i.text)\n",
    "\n",
    "    \n",
    "# Scraping data of GSDP at current price (19-20)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[3]\"):\n",
    "    GSDP_1.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of GSDP at current price (18-19)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[4]\"):\n",
    "    GSDP_2.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Share(18-19)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[5]\"):\n",
    "    Share.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of GDP($ billion)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[6]\"):\n",
    "    GDP.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "StatisticsTime=pd.DataFrame({})\n",
    "StatisticsTime['Rank'] = Rank\n",
    "StatisticsTime['State'] = State\n",
    "StatisticsTime['GSDP at current price (19-20)'] = GSDP_1\n",
    "StatisticsTime['GSDP at current price (18-19)'] = GSDP_2\n",
    "StatisticsTime['Share(18-19)'] = Share\n",
    "StatisticsTime['GDP($ billion)'] = GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the data frame\n",
    "StatisticsTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of trending repositories on Github.com. Url = https://github.com/ You have to find the following details:\n",
    "- A) Repository title\n",
    "- B) Repository description\n",
    "- C) Contributors count\n",
    "- D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://github.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting explore button and clicking on it\n",
    "explore = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details\").click()\n",
    "\n",
    "#Selecting trending option\n",
    "trend_url = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "repo_urls = []\n",
    "rep_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching urls for each repository\n",
    "repository = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']//a\")\n",
    "for i in repository:\n",
    "    repo_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    \n",
    "#Scraping data of repository title\n",
    "title = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    rep_title.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data from every repository page\n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Scraping data of decription\n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath(\"//p[@class='f4 mt-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping data of contributors\n",
    "    try:\n",
    "        cont_tags = driver.find_element_by_xpath(\"//*[contains(text(),'    Contributors ')]\")\n",
    "        Contributors.append(cont_tags.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "        \n",
    "        \n",
    "    #fetching Languages used\n",
    "    try:\n",
    "        for i in driver.find_elements_by_xpath(\"//span[@class='color-text-primary text-bold mr-1']\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Github=pd.DataFrame({})\n",
    "Github['Repository title'] = rep_title\n",
    "Github['Repository description'] = Description\n",
    "Github['Contributors count'] = Contributors\n",
    "Github['Language used'] = Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of top 100 songs on billboard.com. Url = https://www.billboard.com/ You have to find the following details:\n",
    "- A) Song name\n",
    "- B) Artist name\n",
    "- C) Last week rank\n",
    "- D) Peak rank\n",
    "- E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on charts\n",
    "charts=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/header/div/ul/li[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Name = []\n",
    "Artist = []\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting url for top 100 \n",
    "urls = driver.find_element_by_xpath(\"//div[@class='charts-landing first-group']//div//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "#Now we have to extract required info from page url\n",
    "#Scraping data of song names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\"):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of artist names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\"):\n",
    "    Artist.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of last week ranks\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of peak rank\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\"):\n",
    "    Peak_rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of weeks on board\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\"):\n",
    "    Weeks_on_board.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Billboard=pd.DataFrame({})\n",
    "Billboard['Name'] = Name\n",
    "Billboard['Artist'] = Artist\n",
    "Billboard['Last week rank'] = Last_week_rank\n",
    "Billboard['Peak rank'] = Peak_rank\n",
    "Billboard['Weeks on board'] = Weeks_on_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the dataframe\n",
    "Billboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scrape the details of Data science recruiters from naukri.com. Url = https://www.naukri.com/ You have to find the following details:\n",
    "- A) Name\n",
    "- B) Designation\n",
    "- C) Company\n",
    "- D) Skills they hire for\n",
    "- E) Location\n",
    "\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching urls for recruiter page\n",
    "rec = driver.find_element_by_xpath(\"//a[@title='Search Recruiters']\")\n",
    "page_url = rec.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching search button,bar xpath and clicking on it\n",
    "search = driver.find_element_by_xpath(\"//div[@class='inpWrap']//input\") \n",
    "search.send_keys(\"Data science \")           \n",
    "but = driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\").click()     \n",
    "time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")[:50]:\n",
    "    Name.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "#Scraping data of Designation\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")[:50]:\n",
    "    Designation.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping data of company name\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]/a[2]\")[:50]:\n",
    "    Company.append(i.text)\n",
    "time.sleep(5)\n",
    "    \n",
    "#scraping data of Skills \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")[:50]:\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Skills.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Skills.append('-')\n",
    "    time.sleep(5)\n",
    "\n",
    "#Scraping data of locations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]//span\")[:50]:\n",
    "    Location.append(i.text)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Naukri=pd.DataFrame({})\n",
    "Naukri['Name'] = Name\n",
    "Naukri['Designation'] = Designation\n",
    "Naukri['Company'] = Company\n",
    "Naukri['Skills'] = Skills\n",
    "Naukri['Location'] = Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Naukri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 7. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scrape the details of Highest selling novels. Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/ You have to find the following details:\n",
    "- A) Book name\n",
    "- B) Author name\n",
    "- C) Volumes sold\n",
    "- D) Publisher\n",
    "- E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Book_name = []\n",
    "Author = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data of book names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr/td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of author names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author.append('-')\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#Scraping data of volumes sold\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of publisher names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[6]\"):\n",
    "    Genre.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Novel=pd.DataFrame({})\n",
    "Novel['Book Name'] = Book_name\n",
    "Novel['Author'] = Author\n",
    "Novel['Volume sold'] = Volumes_sold\n",
    "Novel['Publisher'] = Publisher\n",
    "Novel['Genre'] = Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Novel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "- A) Name\n",
    "- B) Year span\n",
    "- C) Genre\n",
    "- D) Run time\n",
    "- E) Ratings\n",
    "- F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists.\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\"):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Year span\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='genre']\"):\n",
    "    Genre.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Run time\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='runtime']\"):\n",
    "    Run_time.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Ratings\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']//span[2]\"):\n",
    "    Ratings.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of votes\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='lister-item-content']//p[4]/span[2]\"):\n",
    "    Votes.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "IMDB=pd.DataFrame({})\n",
    "IMDB['Name'] = Name\n",
    "IMDB['Year Span'] = Year_span\n",
    "IMDB['Genre'] = Genre\n",
    "IMDB['Run Time'] = Run_time\n",
    "IMDB['Ratings'] = Ratings\n",
    "IMDB['Votes'] = Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "- A) Dataset name\n",
    "- B) Data type\n",
    "- C) Task\n",
    "- D) Attribute type\n",
    "- E) No of instances\n",
    "- F) No of attribute\n",
    "- G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding view all dataset button\n",
    "view_dataset = driver.find_element_by_xpath(\"//tbody[1]//tr/td[2]/span[2]/a\")    \n",
    "page_url = view_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting page urls containing list of all datasets\n",
    "all_lst = driver.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[2]/p/a\")  \n",
    "lst_url = all_lst.get_attribute(\"href\")           \n",
    "driver.get(lst_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#fetching url for each dataset\n",
    "data_url = driver.find_elements_by_xpath(\"//p[@class='normal']//b/a\")    \n",
    "\n",
    "urls = []     \n",
    "for i in data_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Data_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attr_type = []\n",
    "Instances = []\n",
    "n_attributes = []\n",
    "Year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping requiredinfo from urls\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Scraping Dataset name\n",
    "    try: \n",
    "        ds_name = driver.find_element_by_xpath(\"//span[@class='heading']\")\n",
    "        Data_name.append(ds_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_name.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data type\n",
    "    try:\n",
    "        dtype = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr/td[2]\")\n",
    "        if dtype.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(dtype.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping Attribute type\n",
    "    try:\n",
    "        atype = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[2]\")\n",
    "        if atype.text == \"N/A\": raise NoSuchElementException\n",
    "        Attr_type.append(atype.text)\n",
    "    except NoSuchElementException:\n",
    "        Attr_type.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #scraping Task\n",
    "    try:\n",
    "        task = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[3]/td[2]\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping No of instances\n",
    "    try:\n",
    "        inst = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr/td[4]\")\n",
    "        if inst.text == \"N/A\": raise NoSuchElementException\n",
    "        Instances.append(inst.text)\n",
    "    except NoSuchElementException:\n",
    "        Instances.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping  No of attribute\n",
    "    try:\n",
    "        attr = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[4]\")\n",
    "        if attr.text == \"N/A\": raise NoSuchElementException\n",
    "        n_attributes.append(attr.text)\n",
    "    except NoSuchElementException:\n",
    "        n_attributes.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #scraping year\n",
    "    try:\n",
    "        year = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[6]\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "UCI=pd.DataFrame({})\n",
    "UCI['Data Name'] = Data_name\n",
    "UCI['Data Type'] = Data_type\n",
    "UCI['Task'] = Task\n",
    "UCI['Attribute type'] = Attr_type\n",
    "UCI['No of instance'] = Instances\n",
    "UCI['No of attributes'] = n_attributes\n",
    "UCI['Year'] = Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "UCI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
